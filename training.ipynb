{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check gpu\n",
    "print(tf.test.is_gpu_available(\n",
    "    cuda_only=False, min_cuda_compute_capability=None\n",
    "))\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain = pd.read_csv(\"data-P1.csv\")\n",
    "dftrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest = pd.read_csv(\"data-P2.csv\")\n",
    "dftest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_catch = dftrain[\"code\"].unique().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_col = [\"code\", \"surface\", \"timestep\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_train = dftrain[dftrain.isna().any(axis=1)][\"timestep\"].unique()  # dealing with missing data\n",
    "\n",
    "train_set = dftrain[~dftrain.timestep.isin(missing_train)]\n",
    "train_set = train_set.drop(drop_col, axis=1)  # drop columns\n",
    "\n",
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_test = dftest[dftest.isna().any(axis=1)][\"timestep\"].unique()  # dealing with missing data\n",
    "\n",
    "test_set = dftest[~dftest.timestep.isin(missing_test)]\n",
    "test_set = test_set.drop(drop_col, axis=1)  # drop columns\n",
    "\n",
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_set.to_numpy()[..., :-1]\n",
    "\n",
    "target = train_set.to_numpy()[..., -1]  # target = target[:, np.newaxis]\n",
    "\n",
    "target = target.reshape(-1, n_catch)\n",
    "\n",
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test_set.to_numpy()[..., :-1]\n",
    "\n",
    "target_test = test_set.to_numpy()[..., -1]  # target_test = target_test[:, np.newaxis]\n",
    "\n",
    "target_test = target_test.reshape(-1, n_catch)\n",
    "\n",
    "target_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RS = RobustScaler()\n",
    "\n",
    "train = RS.fit_transform(train)  # train = train[:, np.newaxis, :]\n",
    "\n",
    "train = train.reshape(-1, n_catch, train.shape[-1])\n",
    "\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RS = RobustScaler()\n",
    "\n",
    "test = RS.fit_transform(test)  # test = test[:, np.newaxis, :]\n",
    "\n",
    "test = test.reshape(-1, n_catch, test.shape[-1])\n",
    "\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_net(input_shape):\n",
    "\n",
    "    net = tf.keras.Sequential()\n",
    "\n",
    "    net.add(Bidirectional(LSTM(128, return_sequences=True), input_shape=input_shape))\n",
    "    net.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "    net.add(Dense(32, activation='selu'))\n",
    "    net.add(Dropout(0.2))\n",
    "    net.add(Dense(1))\n",
    "\n",
    "    net.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 300\n",
    "BATCH_SIZE = 512\n",
    "K_FOLD = 5\n",
    "\n",
    "net_path = \"nets/net\"\n",
    "test_preds = []\n",
    "history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():    \n",
    "    \n",
    "    kf = KFold(n_splits=K_FOLD, shuffle=True, random_state=11)\n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(train, target)):  # Cross Validation Training\n",
    "\n",
    "        print(f'</> Training Fold {fold + 1}...')\n",
    "\n",
    "        X_train, X_valid = train[train_idx], train[test_idx]\n",
    "        y_train, y_valid = target[train_idx], target[test_idx]\n",
    "\n",
    "        net = create_net(train.shape[-2:])\n",
    "\n",
    "        scheduler = ExponentialDecay(1e-3, 100*((train.shape[0]*0.8)/BATCH_SIZE), 1e-5)\n",
    "        lr = LearningRateScheduler(scheduler, verbose=1)\n",
    "        \n",
    "        cp = tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=f\"{net_path}_fold{fold + 1}\",\n",
    "            save_weights_only=True,\n",
    "            mode='min',\n",
    "            save_best_only=True)\n",
    "    \n",
    "        history.append(net.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=EPOCH, batch_size=BATCH_SIZE, callbacks=[lr,cp]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hist in history:\n",
    "    tl = hist.history[\"loss\"]\n",
    "    vl = hist.history[\"val_loss\"]\n",
    "    plt.plot(range(len(tl)), tl, vl)\n",
    "    plt.ylim([0,5])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets = []\n",
    "for fold in range(K_FOLD):\n",
    "    net = create_net(train.shape[-2:])\n",
    "    net.load_weights(f\"{net_path}_fold{fold + 1}\")\n",
    "    nets.append(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = np.mean([net.predict(train) for net in nets], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(ypred.size),target.flatten(), ypred.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for loss in [mean_absolute_error, mean_squared_error]:\n",
    "    print(f\"{loss.__name__}: not corrected {loss(np.zeros(target.flatten().shape),target.flatten())}, corrected {loss(ypred.flatten(),target.flatten())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_correct = dftrain.copy()\n",
    "df_correct.loc[~df_correct.timestep.isin(missing_train), \"bias\"]=ypred.flatten()\n",
    "\n",
    "df_correct.to_csv(\"data-corrected-P1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred_test = np.mean([net.predict(test) for net in nets], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(ypred_test.size),target_test.flatten(), ypred_test.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for loss in [mean_absolute_error, mean_squared_error]:\n",
    "    print(f\"{loss.__name__}: not corrected {loss(np.zeros(target_test.flatten().shape),target_test.flatten())}, corrected {loss(ypred_test.flatten(),target_test.flatten())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_correct = dftest.copy()\n",
    "df_correct.loc[~df_correct.timestep.isin(missing_test), \"bias\"]=ypred_test.flatten()\n",
    "\n",
    "df_correct.to_csv(\"data-corrected-P2.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smash-deepl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
